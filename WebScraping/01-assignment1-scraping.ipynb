{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zN9NJCwAwf9e"
   },
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1. Imports and Data Loading\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Sklearn imports for classical models\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# PyTorch imports for LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0pMbw3DwVaq"
   },
   "source": [
    "# Dataset-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-YUWQyjdt5s",
    "outputId": "875cd832-407e-464f-ff04-a8ce4946c389"
   },
   "outputs": [],
   "source": [
    "# Load the Excel dataset (adjust path if needed)\n",
    "file_path = \"Dataset-1.xlsx\"  # Adjust your file path as necessary\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display first few rows (for debugging purposes)\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# =========================================\n",
    "# 2. Data Preprocessing\n",
    "# =========================================\n",
    "\n",
    "# --- Create target label from multiple subject columns ---\n",
    "subject_columns = [\n",
    "    \"Computer Science\", \"Physics\", \"Mathematics\",\n",
    "    \"Statistics\", \"Quantitative Biology\", \"Quantitative Finance\"\n",
    "]\n",
    "\n",
    "def get_subject(row):\n",
    "    for col in subject_columns:\n",
    "        if row[col] == 1:\n",
    "            return col\n",
    "    return \"Unknown\"\n",
    "\n",
    "df[\"target\"] = df.apply(get_subject, axis=1)\n",
    "print(\"\\nUnique targets:\", df[\"target\"].unique())\n",
    "\n",
    "# --- Combine 'TITLE' and 'ABSTRACT' into one text field ---\n",
    "df[\"text\"] = (df[\"TITLE\"].astype(str) + \" \" + df[\"ABSTRACT\"].astype(str)).str.lower()\n",
    "\n",
    "# --- Encode the target labels ---\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"target\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nEncoded classes:\", label_encoder.classes_)\n",
    "\n",
    "# --- Manual splitting per class ---\n",
    "# For each class, split samples into 50% train, 30% validation, 20% test.\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for label in df[\"label\"].unique():\n",
    "    group = df[df[\"label\"] == label]\n",
    "    indices = group.index.tolist()\n",
    "    np.random.shuffle(indices)\n",
    "    n = len(indices)\n",
    "    train_count = int(0.5 * n)\n",
    "    val_count = int(0.3 * n)\n",
    "    train_indices.extend(indices[:train_count])\n",
    "    val_indices.extend(indices[train_count:train_count + val_count])\n",
    "    test_indices.extend(indices[train_count + val_count:])\n",
    "\n",
    "# Create train, validation, and test sets using the computed indices\n",
    "X_train = df.loc[train_indices, \"text\"]\n",
    "y_train = df.loc[train_indices, \"label\"]\n",
    "X_val   = df.loc[val_indices, \"text\"]\n",
    "y_val   = df.loc[val_indices, \"label\"]\n",
    "X_test  = df.loc[test_indices, \"text\"]\n",
    "y_test  = df.loc[test_indices, \"label\"]\n",
    "\n",
    "print(f\"\\nDataset split sizes by manual per-class splitting:\")\n",
    "print(f\"Train: {len(X_train)}, Validation: {len(X_val)}, Test: {len(X_test)}\")\n",
    "\n",
    "# =========================================\n",
    "# 3. Classical Models with TF-IDF Features\n",
    "# =========================================\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf   = vectorizer.transform(X_val)\n",
    "X_test_tfidf  = vectorizer.transform(X_test)\n",
    "\n",
    "# ----- Naive Bayes -----\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "print(\"\\nNaive Bayes Accuracy on Test Set:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# ----- Support Vector Machine (Linear SVC) -----\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"\\nSVM Accuracy on Test Set:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# ----- Random Forest -----\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(\"\\nRandom Forest Accuracy on Test Set:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# =========================================\n",
    "# 4. LSTM Model with PyTorch\n",
    "# =========================================\n",
    "from collections import Counter\n",
    "\n",
    "# --- Build a Vocabulary ---\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()  # simple whitespace tokenization\n",
    "        counter.update(tokens)\n",
    "    # Reserve index 0 for <PAD> and 1 for <UNK>\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary on the training texts\n",
    "vocab = build_vocab(X_train)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nVocabulary Size: {vocab_size}\")\n",
    "\n",
    "# --- Helper Function: Convert Text to Sequence ---\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = text.split()\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# --- PyTorch Dataset for LSTM ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        seq = text_to_sequence(text, self.vocab)\n",
    "        if len(seq) < self.max_len:\n",
    "            seq = seq + [self.vocab[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "max_len = 100  # maximum sequence length\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train, vocab, max_len)\n",
    "val_dataset   = TextDataset(X_val, y_val, vocab, max_len)\n",
    "test_dataset  = TextDataset(X_test, y_test, vocab, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Define the LSTM Classifier Model ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, max_len, embed_dim]\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# --- Set up CUDA ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_epochs = 50   # maximum epochs\n",
    "learning_rate = 0.001\n",
    "patience = 5      # early stopping patience\n",
    "\n",
    "lstm_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, bidirectional=True)\n",
    "lstm_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nTraining LSTM Model\")\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    # Validation Phase\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = lstm_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} -- Train Loss: {train_loss:.4f} -- Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Check with Patience\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Validation loss did not improve for 5 consecutive epochs; early stopping.\")\n",
    "            break\n",
    "\n",
    "# --- Evaluate the LSTM Model on the Test Set ---\n",
    "lstm_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = lstm_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nLSTM Test Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYpjnzy4P2rJ"
   },
   "source": [
    "# A2D2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vunDaU0sO3lx",
    "outputId": "ae0d8ce2-c75c-42e6-fd12-5c806d4f7633"
   },
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Text Classification on A2D2.xlsx\n",
    "# =============================================\n",
    "\n",
    "\n",
    "# Load the Excel dataset \"A2D2.xlsx\"\n",
    "file_path = \"A2D2.xlsx\"  # Adjust the path if necessary\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(\"First few rows of A2D2.xlsx:\")\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Data Preprocessing\n",
    "# ------------------------------\n",
    "# For A2D2, we use the \"content\" column as our text feature.\n",
    "df[\"text\"] = df[\"Content\"].astype(str).str.lower()\n",
    "\n",
    "# Encode target labels from the \"domain\" column\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"Domain\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nEncoded classes:\", label_encoder.classes_)\n",
    "\n",
    "# --- Manual per-class splitting ---\n",
    "# For each class, split samples into 50% train, 30% validation, and 20% test.\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for lbl in df[\"label\"].unique():\n",
    "    group = df[df[\"label\"] == lbl]\n",
    "    indices = group.index.tolist()\n",
    "    np.random.shuffle(indices)\n",
    "    n = len(indices)\n",
    "    train_count = int(0.5 * n)\n",
    "    val_count = int(0.3 * n)\n",
    "    train_indices.extend(indices[:train_count])\n",
    "    val_indices.extend(indices[train_count:train_count + val_count])\n",
    "    test_indices.extend(indices[train_count + val_count:])\n",
    "\n",
    "# Create train, validation, and test sets\n",
    "X_train_text = df.loc[train_indices, \"text\"]\n",
    "y_train = df.loc[train_indices, \"label\"]\n",
    "X_val_text = df.loc[val_indices, \"text\"]\n",
    "y_val = df.loc[val_indices, \"label\"]\n",
    "X_test_text = df.loc[test_indices, \"text\"]\n",
    "y_test = df.loc[test_indices, \"label\"]\n",
    "\n",
    "print(f\"\\nManual split sizes: Train={len(X_train_text)}, Val={len(X_val_text)}, Test={len(X_test_text)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Classical Models with TF-IDF Features\n",
    "# ------------------------------\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf  = vectorizer.transform(X_test_text)\n",
    "# (You may also vectorize X_val_text if desired)\n",
    "\n",
    "# ----- Naive Bayes -----\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "print(\"\\nNaive Bayes Accuracy on Test Set:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# ----- Support Vector Machine (LinearSVC) -----\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"\\nSVM Accuracy on Test Set:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# ----- Random Forest -----\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(\"\\nRandom Forest Accuracy on Test Set:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_, zero_division=0))\n",
    "\n",
    "# ------------------------------\n",
    "# 4. LSTM Model with PyTorch\n",
    "# ------------------------------\n",
    "from collections import Counter\n",
    "\n",
    "# --- Build a Vocabulary ---\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()  # simple whitespace tokenization\n",
    "        counter.update(tokens)\n",
    "    # Reserve index 0 for <PAD> and index 1 for <UNK>\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary on training texts\n",
    "vocab = build_vocab(X_train_text)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nVocabulary Size: {vocab_size}\")\n",
    "\n",
    "# --- Helper Function: Convert Text to Sequence ---\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = text.split()\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# --- PyTorch Dataset for LSTM ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        seq = text_to_sequence(text, self.vocab)\n",
    "        # Pad or truncate the sequence to fixed length\n",
    "        if len(seq) < self.max_len:\n",
    "            seq = seq + [self.vocab[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "max_len = 100  # Maximum sequence length\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDataset(X_train_text, y_train, vocab, max_len)\n",
    "val_dataset   = TextDataset(X_val_text, y_val, vocab, max_len)\n",
    "test_dataset  = TextDataset(X_test_text, y_test, vocab, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Define the LSTM Classifier Model ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, max_len, embed_dim]\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# --- Set up CUDA ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Hyperparameters for LSTM\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_epochs = 50   # Maximum epochs\n",
    "learning_rate = 0.001\n",
    "patience = 5      # Early stopping patience\n",
    "\n",
    "lstm_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, bidirectional=True)\n",
    "lstm_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"\\nTraining LSTM Model with Early Stopping (patience = 5)...\")\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    # Validation Phase\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = lstm_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} -- Train Loss: {train_loss:.4f} -- Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Validation loss did not improve for 5 consecutive epochs; early stopping.\")\n",
    "            break\n",
    "\n",
    "# --- Evaluate the LSTM Model on the Test Set ---\n",
    "lstm_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = lstm_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nLSTM Test Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X47i479Czj6i"
   },
   "source": [
    "# Dataset-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jfQ6PdNgZDm4",
    "outputId": "868434f9-ef9a-4493-cf16-621a4cf0ddfe"
   },
   "outputs": [],
   "source": [
    "# Load the Excel dataset (adjust path if needed)\n",
    "file_path = \"Dataset-2.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display first few rows for debugging\n",
    "print(\"First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# =========================================\n",
    "# 2. Data Preprocessing for All Models\n",
    "# =========================================\n",
    "\n",
    "# Combine 'Title' and 'Content' into a single text field (and lowercase it)\n",
    "df[\"text\"] = (df[\"Title\"].astype(str) + \" \" + df[\"Content\"].astype(str)).str.lower()\n",
    "\n",
    "# Encode target labels from the \"Domain\" column\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"Domain\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"\\nEncoded classes:\", label_encoder.classes_)\n",
    "\n",
    "# --- Manual per-class splitting into 50% train, 30% validation, 20% test ---\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "test_indices = []\n",
    "\n",
    "for lbl in df[\"label\"].unique():\n",
    "    group = df[df[\"label\"] == lbl]\n",
    "    indices = group.index.tolist()\n",
    "    np.random.shuffle(indices)\n",
    "    n = len(indices)\n",
    "    train_count = int(0.5 * n)\n",
    "    val_count = int(0.3 * n)\n",
    "    # The remaining samples go to test (20% or remainder)\n",
    "    train_indices.extend(indices[:train_count])\n",
    "    val_indices.extend(indices[train_count:train_count + val_count])\n",
    "    test_indices.extend(indices[train_count + val_count:])\n",
    "\n",
    "# Create train, validation, and test sets\n",
    "X_train_text = df.loc[train_indices, \"text\"]\n",
    "y_train = df.loc[train_indices, \"label\"]\n",
    "X_val_text = df.loc[val_indices, \"text\"]\n",
    "y_val = df.loc[val_indices, \"label\"]\n",
    "X_test_text = df.loc[test_indices, \"text\"]\n",
    "y_test = df.loc[test_indices, \"label\"]\n",
    "\n",
    "print(f\"\\nManual split sizes: Train={len(X_train_text)}, Val={len(X_val_text)}, Test={len(X_test_text)}\")\n",
    "\n",
    "# =========================================\n",
    "# 3. Classical Models using TF-IDF Features\n",
    "# =========================================\n",
    "\n",
    "# Vectorize text with TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf  = vectorizer.transform(X_test_text)\n",
    "\n",
    "# ----- Naive Bayes -----\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "print(\"\\nNaive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_))\n",
    "\n",
    "# ----- Support Vector Machine (Linear SVC) -----\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "print(\"\\nSVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(classification_report(y_test, y_pred_svm, target_names=label_encoder.classes_))\n",
    "\n",
    "# ----- Random Forest -----\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "print(\"\\nRandom Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n",
    "\n",
    "# =========================================\n",
    "# 4. LSTM Model with PyTorch\n",
    "# =========================================\n",
    "from collections import Counter\n",
    "\n",
    "# --- Build a Vocabulary ---\n",
    "def build_vocab(texts, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        tokens = text.split()  # simple whitespace tokenization\n",
    "        counter.update(tokens)\n",
    "    # Reserve indices: 0 for <PAD> and 1 for <UNK>\n",
    "    vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for token, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary on training texts\n",
    "vocab = build_vocab(X_train_text)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"\\nVocabulary Size: {vocab_size}\")\n",
    "\n",
    "# --- Helper Function: Convert Text to Sequence ---\n",
    "def text_to_sequence(text, vocab):\n",
    "    tokens = text.split()\n",
    "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "\n",
    "# --- PyTorch Dataset for LSTM ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=100):\n",
    "        self.texts = texts.reset_index(drop=True)\n",
    "        self.labels = labels.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        seq = text_to_sequence(text, self.vocab)\n",
    "        # Pad or truncate the sequence to fixed length\n",
    "        if len(seq) < self.max_len:\n",
    "            seq = seq + [self.vocab[\"<PAD>\"]] * (self.max_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:self.max_len]\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Create datasets and DataLoaders for train, validation, and test\n",
    "max_len = 100  # maximum sequence length\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = TextDataset(X_train_text, y_train, vocab, max_len)\n",
    "val_dataset   = TextDataset(X_val_text, y_val, vocab, max_len)\n",
    "test_dataset  = TextDataset(X_test_text, y_test, vocab, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- Define the LSTM Classifier Model ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, num_layers=1, bidirectional=True):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=bidirectional)\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(lstm_output_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch, max_len, embed_dim]\n",
    "        lstm_out, (h_n, _) = self.lstm(embedded)\n",
    "        # For bidirectional LSTM, concatenate final forward and backward hidden states\n",
    "        if self.lstm.bidirectional:\n",
    "            h = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        out = self.fc(h)\n",
    "        return out\n",
    "\n",
    "# --- Set up CUDA ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Hyperparameters for LSTM\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_epochs = 50   # maximum epochs\n",
    "learning_rate = 0.001\n",
    "patience = 5      # early stopping patience\n",
    "\n",
    "lstm_model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, num_classes, bidirectional=True)\n",
    "lstm_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# --- Training Loop with Early Stopping ---\n",
    "print(\"\\nTraining LSTM Model\")\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    lstm_model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "\n",
    "    # Validation Phase\n",
    "    lstm_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = lstm_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    val_loss /= len(val_dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} -- Train Loss: {train_loss:.4f} -- Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Check with Patience\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Validation loss did not improve for 5 consecutive epochs; early stopping.\")\n",
    "            break\n",
    "\n",
    "# --- Evaluate the LSTM Model on the Test Set ---\n",
    "lstm_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = lstm_model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nLSTM Test Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
