{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T07:27:03.186995Z",
     "iopub.status.busy": "2025-04-19T07:27:03.186298Z",
     "iopub.status.idle": "2025-04-19T07:28:13.251748Z",
     "shell.execute_reply": "2025-04-19T07:28:13.250930Z",
     "shell.execute_reply.started": "2025-04-19T07:27:03.186969Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pip install torch transformers scikit-learn pandas tqdm lime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:34:55.161928Z",
     "iopub.status.busy": "2025-04-19T07:34:55.161607Z",
     "iopub.status.idle": "2025-04-19T07:34:55.167289Z",
     "shell.execute_reply": "2025-04-19T07:34:55.166496Z",
     "shell.execute_reply.started": "2025-04-19T07:34:55.161909Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== Config ==========\n",
    "USE_FOCAL_LOSS = False\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "LR = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:34:57.193057Z",
     "iopub.status.busy": "2025-04-19T07:34:57.192788Z",
     "iopub.status.idle": "2025-04-19T07:34:57.198404Z",
     "shell.execute_reply": "2025-04-19T07:34:57.197820Z",
     "shell.execute_reply.started": "2025-04-19T07:34:57.193031Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== Dataset ==========\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_cols):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_cols = label_cols\n",
    "        self.labels = self.df[label_cols].values.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx]),\n",
    "            'raw_text': text\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T07:34:57.981758Z",
     "iopub.status.busy": "2025-04-19T07:34:57.981178Z",
     "iopub.status.idle": "2025-04-19T07:34:57.987953Z",
     "shell.execute_reply": "2025-04-19T07:34:57.987314Z",
     "shell.execute_reply.started": "2025-04-19T07:34:57.981737Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========== Model ==========\n",
    "class XLMREmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "# ========== Focal Loss ==========\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0):\n",
    "    BCE = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE)\n",
    "    return (alpha * (1 - pt) ** gamma * BCE).mean()\n",
    "\n",
    "# ========== Metrics ==========\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = (torch.sigmoid(preds) > 0.5).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    return {\n",
    "        'f1_macro': f1_score(labels, preds, average='macro', zero_division=0),\n",
    "        'precision_macro': precision_score(labels, preds, average='macro', zero_division=0),\n",
    "        'recall_macro': recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:13:16.666186Z",
     "iopub.status.busy": "2025-04-19T09:13:16.665590Z",
     "iopub.status.idle": "2025-04-19T09:13:16.673147Z",
     "shell.execute_reply": "2025-04-19T09:13:16.672464Z",
     "shell.execute_reply.started": "2025-04-19T09:13:16.666165Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== Training ==========\n",
    "def train_model(model, train_loader, val_loader, label_cols,USE_FOCAL_LOSS=False):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = focal_loss if USE_FOCAL_LOSS else nn.BCEWithLogitsLoss()\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        evaluate_model(model, val_loader, label_cols)\n",
    "\n",
    "# ========== Evaluation ==========\n",
    "def evaluate_model(model, val_loader, label_cols):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            all_preds.append(logits)\n",
    "            all_labels.append(labels)\n",
    "    preds = torch.cat(all_preds)\n",
    "    labels = torch.cat(all_labels)\n",
    "    metrics = compute_metrics(preds, labels)\n",
    "    print(\"Validation Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-19T08:48:11.275153Z",
     "iopub.status.busy": "2025-04-19T08:48:11.274388Z",
     "iopub.status.idle": "2025-04-19T08:48:11.284071Z",
     "shell.execute_reply": "2025-04-19T08:48:11.283485Z",
     "shell.execute_reply.started": "2025-04-19T08:48:11.275126Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== LIME ==========\n",
    "def explain_prediction(model, tokenizer, text, label_names):\n",
    "    model.eval()\n",
    "    def predict_proba(texts):\n",
    "        model.eval()\n",
    "        outputs = []\n",
    "        for t in texts:\n",
    "            encoding = tokenizer(t, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=MAX_LENGTH).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                logits = model(**encoding)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "            outputs.append(probs)\n",
    "        return np.array(outputs)\n",
    "    all_labels = list(range(len(label_names)))\n",
    "    explainer = LimeTextExplainer(class_names=label_names)\n",
    "    exp = explainer.explain_instance(text, predict_proba, num_features=10, labels=all_labels)\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "# ========== Attention Visualization ==========\n",
    "def plot_attention(model, tokenizer, text):\n",
    "    encoding = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=MAX_LENGTH)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "    attention_mask = encoding['attention_mask'][0]\n",
    "\n",
    "    # Move to device\n",
    "    encoding = {k: v.to(DEVICE) for k, v in encoding.items()}\n",
    "\n",
    "    # Get attentions from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model.encoder(**encoding, output_attentions=True)\n",
    "\n",
    "    # Layer 0, Head 0\n",
    "    attentions = outputs.attentions[0][0, 0]  # (seq_len, seq_len)\n",
    "    token_scores = attentions.sum(dim=0).cpu().numpy()  # total attention received per token\n",
    "\n",
    "    # Convert token IDs to tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Filter: remove special tokens and padding\n",
    "    filtered_scores = []\n",
    "    filtered_tokens = []\n",
    "    for score, token, mask in zip(token_scores, tokens, attention_mask):\n",
    "        if mask.item() == 1 and token not in tokenizer.all_special_tokens:\n",
    "            filtered_scores.append(score)\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.bar(range(len(filtered_tokens)), filtered_scores, color='skyblue')\n",
    "    plt.xticks(range(len(filtered_tokens)), filtered_tokens, rotation='vertical')\n",
    "    plt.title(\"Attention Scores (Layer 0, Head 0)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T08:12:00.054934Z",
     "iopub.status.busy": "2025-04-19T08:12:00.054616Z",
     "iopub.status.idle": "2025-04-19T08:12:00.060137Z",
     "shell.execute_reply": "2025-04-19T08:12:00.059200Z",
     "shell.execute_reply.started": "2025-04-19T08:12:00.054913Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(csv_path, label_cols, batch_size=BATCH_SIZE):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_data = EmotionDataset(train_df, tokenizer, label_cols)\n",
    "    val_data = EmotionDataset(val_df, tokenizer, label_cols)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, tokenizer, train_df, val_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With BCE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T08:14:57.662643Z",
     "iopub.status.busy": "2025-04-19T08:14:57.662350Z",
     "iopub.status.idle": "2025-04-19T08:27:33.416606Z",
     "shell.execute_reply": "2025-04-19T08:27:33.416027Z",
     "shell.execute_reply.started": "2025-04-19T08:14:57.662622Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ========== Run English Model ==========\n",
    "english_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "eng_train_loader,eng_val_loader,eng_tokenizer,eng_train_df,eng_val_df = get_dataloaders(\"/kaggle/input/languages/eng.csv\",english_labels)\n",
    "eng_model = XLMREmotionClassifier(num_labels=len(english_labels))\n",
    "train_model(eng_model, eng_train_loader, eng_val_loader, english_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T08:41:31.639274Z",
     "iopub.status.busy": "2025-04-19T08:41:31.639035Z",
     "iopub.status.idle": "2025-04-19T08:42:16.831250Z",
     "shell.execute_reply": "2025-04-19T08:42:16.830633Z",
     "shell.execute_reply.started": "2025-04-19T08:41:31.639259Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# LIME + attention on one sample\n",
    "eng_sample_text = eng_val_df.iloc[0]['text']\n",
    "eng_sample_labels = eng_val_df.iloc[0][english_labels].to_dict()\n",
    "\n",
    "print(\"\\nSample Text:\", eng_sample_text)\n",
    "print(\"Original Labels:\", eng_sample_labels)\n",
    "explain_prediction(eng_model, eng_tokenizer, eng_sample_text, english_labels)\n",
    "plot_attention(eng_model, eng_tokenizer, eng_sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with FOCAL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:14:36.512938Z",
     "iopub.status.busy": "2025-04-19T09:14:36.512435Z",
     "iopub.status.idle": "2025-04-19T09:27:14.327180Z",
     "shell.execute_reply": "2025-04-19T09:27:14.326402Z",
     "shell.execute_reply.started": "2025-04-19T09:14:36.512916Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== Run English Model ==========\n",
    "english_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n",
    "eng_train_loader,eng_val_loader,eng_tokenizer,eng_train_df,eng_val_df = get_dataloaders(\"/kaggle/input/languages/eng.csv\",english_labels)\n",
    "eng_model = XLMREmotionClassifier(num_labels=len(english_labels))\n",
    "train_model(eng_model, eng_train_loader, eng_val_loader, english_labels,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:30:15.191120Z",
     "iopub.status.busy": "2025-04-19T09:30:15.190824Z",
     "iopub.status.idle": "2025-04-19T09:31:00.502730Z",
     "shell.execute_reply": "2025-04-19T09:31:00.502099Z",
     "shell.execute_reply.started": "2025-04-19T09:30:15.191095Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# LIME + attention on one sample\n",
    "eng_sample_text = eng_val_df.iloc[0]['text']\n",
    "eng_sample_labels = eng_val_df.iloc[0][english_labels].to_dict()\n",
    "\n",
    "print(\"\\nSample Text:\", eng_sample_text)\n",
    "print(\"Original Labels:\", eng_sample_labels)\n",
    "explain_prediction(eng_model, eng_tokenizer, eng_sample_text, english_labels)\n",
    "plot_attention(eng_model, eng_tokenizer, eng_sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with BCE_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T08:48:28.513915Z",
     "iopub.status.busy": "2025-04-19T08:48:28.513598Z",
     "iopub.status.idle": "2025-04-19T08:57:33.684341Z",
     "shell.execute_reply": "2025-04-19T08:57:33.683698Z",
     "shell.execute_reply.started": "2025-04-19T08:48:28.513894Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ========== Run English Model ==========\n",
    "non_english_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise', 'disgust']\n",
    "esp_train_loader,esp_val_loader,esp_tokenizer,esp_train_df,esp_val_df = get_dataloaders(\"/kaggle/input/languages/esp.csv\",non_english_labels)\n",
    "esp_model = XLMREmotionClassifier(num_labels=len(non_english_labels))\n",
    "train_model(esp_model, esp_train_loader, esp_val_loader, non_english_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-19T09:04:46.715789Z",
     "iopub.status.busy": "2025-04-19T09:04:46.715492Z",
     "iopub.status.idle": "2025-04-19T09:05:31.801633Z",
     "shell.execute_reply": "2025-04-19T09:05:31.801040Z",
     "shell.execute_reply.started": "2025-04-19T09:04:46.715768Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# LIME + attention on one sample\n",
    "esp_sample_text = esp_val_df.iloc[0]['text']\n",
    "esp_sample_labels = esp_val_df.iloc[0][non_english_labels].to_dict()\n",
    "\n",
    "print(\"\\nSample Text:\", esp_sample_text)\n",
    "print(\"Original Labels:\", esp_sample_labels)\n",
    "explain_prediction(esp_model, esp_tokenizer, esp_sample_text, non_english_labels)\n",
    "plot_attention(esp_model, esp_tokenizer, esp_sample_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with FOCAL_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:37:31.458422Z",
     "iopub.status.busy": "2025-04-19T09:37:31.457781Z",
     "iopub.status.idle": "2025-04-19T09:46:36.627585Z",
     "shell.execute_reply": "2025-04-19T09:46:36.626957Z",
     "shell.execute_reply.started": "2025-04-19T09:37:31.458402Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== Run EFOCAL lossnglish Model ==========\n",
    "non_english_labels = ['anger', 'fear', 'joy', 'sadness', 'surprise', 'disgust']\n",
    "esp_train_loader,esp_val_loader,esp_tokenizer,esp_train_df,esp_val_df = get_dataloaders(\"/kaggle/input/languages/esp.csv\",non_english_labels)\n",
    "esp_model = XLMREmotionClassifier(num_labels=len(non_english_labels))\n",
    "train_model(esp_model, esp_train_loader, esp_val_loader, non_english_labels,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-19T09:46:56.349758Z",
     "iopub.status.busy": "2025-04-19T09:46:56.349096Z",
     "iopub.status.idle": "2025-04-19T09:47:41.539741Z",
     "shell.execute_reply": "2025-04-19T09:47:41.539118Z",
     "shell.execute_reply.started": "2025-04-19T09:46:56.349734Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# LIME + attention on one sample\n",
    "esp_sample_text = esp_val_df.iloc[0]['text']\n",
    "esp_sample_labels = esp_val_df.iloc[0][non_english_labels].to_dict()\n",
    "\n",
    "print(\"\\nSample Text:\", esp_sample_text)\n",
    "print(\"Original Labels:\", esp_sample_labels)\n",
    "explain_prediction(esp_model, esp_tokenizer, esp_sample_text, non_english_labels)\n",
    "plot_attention(esp_model, esp_tokenizer, esp_sample_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:03:23.068414Z",
     "iopub.status.busy": "2025-04-25T07:03:23.067678Z",
     "iopub.status.idle": "2025-04-25T07:03:25.941135Z",
     "shell.execute_reply": "2025-04-25T07:03:25.940558Z",
     "shell.execute_reply.started": "2025-04-25T07:03:23.068387Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========== Config ==========\n",
    "MAX_LENGTH  = 128\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 20\n",
    "LR          = 2e-5\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== COMET-ATOMIC Setup (public model) ==========\n",
    "COMET_REPO      = \"mismayil/comet-bart-ai2\"\n",
    "comet_tokenizer = AutoTokenizer.from_pretrained(COMET_REPO)\n",
    "comet_model     = AutoModelForSeq2SeqLM.from_pretrained(COMET_REPO).to(DEVICE)\n",
    "\n",
    "def generate_comet_context(text: str, relations=[\"xIntent\",\"xNeed\"], max_new_tokens=20) -> str:\n",
    "    contexts = []\n",
    "    for rel in relations:\n",
    "        prompt = f\"{text} <{rel}>\"\n",
    "        inputs = comet_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        outs   = comet_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        ctx    = comet_tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        contexts.append(f\"[{rel}]: {ctx}\")\n",
    "    return \" \".join(contexts)\n",
    "# ========== Dataset Classes ==========\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_cols, max_length=MAX_LENGTH):\n",
    "        # copy to avoid modifying original\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        # robustly convert label columns to numeric 0/1\n",
    "        self.df[label_cols] = (\n",
    "            self.df[label_cols]\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")  # non-numeric→NaN\n",
    "            .fillna(0)                               # NaN→0\n",
    "            .astype(int)                             # to int\n",
    "        )\n",
    "        self.df[label_cols] = self.df[label_cols].astype(float)\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.label_cols = label_cols\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        enc  = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        labels = torch.tensor(\n",
    "            self.df.loc[idx, self.label_cols].values,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\":       enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\":  enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":          labels\n",
    "        }\n",
    "class EmotionDatasetWithComet(EmotionDataset):\n",
    "    def __init__(self, df, tokenizer, label_cols,\n",
    "                 max_length=MAX_LENGTH, use_comet=False, comet_relations=[\"xIntent\",\"xNeed\"]):\n",
    "        super().__init__(df, tokenizer, label_cols, max_length)\n",
    "        self.use_comet       = use_comet\n",
    "        self.comet_relations = comet_relations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        enc  = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        # FIX: force labels to be a float32 numpy array\n",
    "        label_array = self.df.loc[idx, self.label_cols].to_numpy(dtype=np.float32)\n",
    "        labels      = torch.from_numpy(label_array)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":         labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:04:06.292459Z",
     "iopub.status.busy": "2025-04-25T07:04:06.292179Z",
     "iopub.status.idle": "2025-04-25T07:04:06.314152Z",
     "shell.execute_reply": "2025-04-25T07:04:06.313320Z",
     "shell.execute_reply.started": "2025-04-25T07:04:06.292439Z"
    }
   },
   "outputs": [],
   "source": [
    "english_labels = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\"]\n",
    "train_loader_eng, val_loader_eng, tokenizer, train_df_eng, val_df_eng = \\\n",
    "    get_dataloaders(\"eng.csv\", english_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:05:42.390319Z",
     "iopub.status.busy": "2025-04-25T07:05:42.390069Z",
     "iopub.status.idle": "2025-04-25T07:05:42.394870Z",
     "shell.execute_reply": "2025-04-25T07:05:42.394176Z",
     "shell.execute_reply.started": "2025-04-25T07:05:42.390302Z"
    }
   },
   "outputs": [],
   "source": [
    "# ========== DataLoader Helper ==========\n",
    "def get_dataloaders(csv_path, label_cols, batch_size=BATCH_SIZE):\n",
    "    df        = pd.read_csv(csv_path)\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    tr_df, vl_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    tr_loader = DataLoader(EmotionDataset(tr_df, tokenizer, label_cols),\n",
    "                           batch_size=batch_size, shuffle=True)\n",
    "    vl_loader = DataLoader(EmotionDataset(vl_df, tokenizer, label_cols),\n",
    "                           batch_size=batch_size)\n",
    "    return tr_loader, vl_loader, tokenizer, tr_df, vl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:05:44.179112Z",
     "iopub.status.busy": "2025-04-25T07:05:44.178399Z",
     "iopub.status.idle": "2025-04-25T07:05:48.460104Z",
     "shell.execute_reply": "2025-04-25T07:05:48.459541Z",
     "shell.execute_reply.started": "2025-04-25T07:05:44.179084Z"
    }
   },
   "outputs": [],
   "source": [
    "english_labels = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\"]\n",
    "train_loader_eng, val_loader_eng, tokenizer, train_df_eng, val_df_eng = \\\n",
    "    get_dataloaders(\"/kaggle/input/languages/esp.csv\", english_labels)\n",
    "\n",
    "train_loader_eng = DataLoader(\n",
    "    EmotionDatasetWithComet(train_df_eng, tokenizer, english_labels, use_comet=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:06:03.932533Z",
     "iopub.status.busy": "2025-04-25T07:06:03.932034Z",
     "iopub.status.idle": "2025-04-25T07:06:03.937040Z",
     "shell.execute_reply": "2025-04-25T07:06:03.936348Z",
     "shell.execute_reply.started": "2025-04-25T07:06:03.932503Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T07:06:47.170389Z",
     "iopub.status.busy": "2025-04-25T07:06:47.170116Z",
     "iopub.status.idle": "2025-04-25T07:30:14.021011Z",
     "shell.execute_reply": "2025-04-25T07:30:14.020455Z",
     "shell.execute_reply.started": "2025-04-25T07:06:47.170369Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========== Config ==========\n",
    "MAX_LENGTH  = 128\n",
    "BATCH_SIZE  = 16\n",
    "EPOCHS      = 20\n",
    "LR          = 2e-5\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ========== COMET-ATOMIC Setup (public model) ==========\n",
    "COMET_REPO      = \"mismayil/comet-bart-ai2\"\n",
    "comet_tokenizer = AutoTokenizer.from_pretrained(COMET_REPO)\n",
    "comet_model     = AutoModelForSeq2SeqLM.from_pretrained(COMET_REPO).to(DEVICE)\n",
    "\n",
    "def generate_comet_context(text: str, relations=[\"xIntent\",\"xNeed\"], max_new_tokens=20) -> str:\n",
    "    contexts = []\n",
    "    for rel in relations:\n",
    "        prompt = f\"{text} <{rel}>\"\n",
    "        inputs = comet_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        outs   = comet_model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "        ctx    = comet_tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "        contexts.append(f\"[{rel}]: {ctx}\")\n",
    "    return \" \".join(contexts)\n",
    "\n",
    "# ========== Dataset Classes ==========\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, label_cols, max_length=MAX_LENGTH):\n",
    "        # copy to avoid modifying original\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        # robustly convert label columns to numeric 0/1\n",
    "        self.df[label_cols] = (\n",
    "            self.df[label_cols]\n",
    "            .apply(pd.to_numeric, errors=\"coerce\")  # non-numeric→NaN\n",
    "            .fillna(0)                               # NaN→0\n",
    "            .astype(int)                             # to int\n",
    "        )\n",
    "        self.df[label_cols] = self.df[label_cols].astype(float)\n",
    "        self.tokenizer  = tokenizer\n",
    "        self.label_cols = label_cols\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        enc  = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        labels = torch.tensor(\n",
    "            self.df.loc[idx, self.label_cols].values,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\":       enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\":  enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":          labels\n",
    "        }\n",
    "\n",
    "class EmotionDatasetWithComet(EmotionDataset):\n",
    "    def __init__(self, df, tokenizer, label_cols,\n",
    "                 max_length=MAX_LENGTH, use_comet=False, comet_relations=[\"xIntent\",\"xNeed\"]):\n",
    "        super().__init__(df, tokenizer, label_cols, max_length)\n",
    "        self.use_comet       = use_comet\n",
    "        self.comet_relations = comet_relations\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        enc  = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        # FIX: force labels to be a float32 numpy array\n",
    "        label_array = self.df.loc[idx, self.label_cols].to_numpy(dtype=np.float32)\n",
    "        labels      = torch.from_numpy(label_array)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\":      enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\":         labels\n",
    "        }\n",
    "\n",
    "# ========== Loss Functions ==========\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0):\n",
    "    bce = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n",
    "    pt  = torch.exp(-bce)\n",
    "    return (alpha * (1 - pt) ** gamma * bce).mean()\n",
    "\n",
    "class LabelSmoothingBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        assert 0.0 <= smoothing < 1.0\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        smooth = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
    "        return nn.functional.binary_cross_entropy_with_logits(logits, smooth)\n",
    "\n",
    "def combined_bce_focal_loss(logits, targets, alpha=0.5, focal_alpha=0.25, focal_gamma=2.0):\n",
    "    bce    = nn.functional.binary_cross_entropy_with_logits(logits, targets)\n",
    "    bce_ex = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    pt     = torch.exp(-bce_ex)\n",
    "    focal  = (focal_alpha * (1 - pt) ** focal_gamma * bce_ex).mean()\n",
    "    return alpha * bce + (1 - alpha) * focal\n",
    "\n",
    "# ========== Metrics ==========\n",
    "def compute_metrics(preds, labels):\n",
    "    preds  = (torch.sigmoid(preds) > 0.5).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    return {\n",
    "        \"f1_macro\":       f1_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"precision_macro\":precision_score(labels, preds, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\":   recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "    }\n",
    "\n",
    "# ========== Model ==========\n",
    "class XLMREmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.encoder    = XLMRobertaModel.from_pretrained(\"xlm-roberta-base\")\n",
    "        self.classifier = nn.Linear(self.encoder.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out     = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_tok = out.last_hidden_state[:, 0, :]\n",
    "        logits  = self.classifier(cls_tok)\n",
    "        return logits\n",
    "\n",
    "# ========== Training Loop ==========\n",
    "def train_model(model, train_loader, val_loader, label_cols,\n",
    "                loss_type=\"bce\", loss_kwargs=None, lr=LR, device=DEVICE):\n",
    "    loss_kwargs = loss_kwargs or {}\n",
    "    if   loss_type==\"bce\":     loss_fn = nn.BCEWithLogitsLoss()\n",
    "    elif loss_type==\"focal\":   loss_fn = focal_loss\n",
    "    elif loss_type==\"lsbce\":   loss_fn = LabelSmoothingBCEWithLogitsLoss(**loss_kwargs)\n",
    "    elif loss_type==\"combined\":\n",
    "        loss_fn = lambda lg, y: combined_bce_focal_loss(lg, y, **loss_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss_type {loss_type}\")\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            ids   = batch[\"input_ids\"].to(device)\n",
    "            mask  = batch[\"attention_mask\"].to(device)\n",
    "            labs  = batch[\"labels\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(ids, mask)\n",
    "            loss   = loss_fn(logits, labs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} — Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids     = batch[\"input_ids\"].to(device)\n",
    "            mask    = batch[\"attention_mask\"].to(device)\n",
    "            labs    = batch[\"labels\"].to(device)\n",
    "            logits  = model(ids, mask)\n",
    "            all_preds.append(logits)\n",
    "            all_labels.append(labs)\n",
    "    preds  = torch.cat(all_preds)\n",
    "    labels = torch.cat(all_labels)\n",
    "    metrics= compute_metrics(preds, labels)\n",
    "    print(\"Validation Metrics:\", metrics)\n",
    "    return metrics\n",
    "\n",
    "# ========== Interpretability ==========\n",
    "def explain_prediction(model, tokenizer, text, label_names):\n",
    "    model.eval()\n",
    "    def predict_proba(texts):\n",
    "        outs = []\n",
    "        for t in texts:\n",
    "            enc = tokenizer(t, return_tensors=\"pt\",\n",
    "                             padding=\"max_length\", truncation=True,\n",
    "                             max_length=MAX_LENGTH).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                lg = model(**enc)\n",
    "            outs.append(torch.sigmoid(lg).cpu().numpy()[0])\n",
    "        return np.array(outs)\n",
    "\n",
    "    explainer = LimeTextExplainer(class_names=label_names)\n",
    "    exp       = explainer.explain_instance(text, predict_proba,\n",
    "                                           num_features=10,\n",
    "                                           labels=list(range(len(label_names))))\n",
    "    exp.show_in_notebook()\n",
    "\n",
    "def plot_attention(model, tokenizer, text):\n",
    "    enc   = tokenizer(text, return_tensors=\"pt\",\n",
    "                      padding=\"max_length\", truncation=True,\n",
    "                      max_length=MAX_LENGTH)\n",
    "    input_ids      = enc[\"input_ids\"][0]\n",
    "    attention_mask = enc[\"attention_mask\"][0]\n",
    "    enc = {k:v.to(DEVICE) for k,v in enc.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model.encoder(**enc, output_attentions=True)\n",
    "    attn    = out.attentions[0][0,0]\n",
    "    tokens  = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    scores  = attn[0].cpu()\n",
    "    filtered= [(s,t) for s,t,m in zip(scores, tokens, attention_mask) if m==1 and t not in tokenizer.all_special_tokens]\n",
    "    scores, tokens = zip(*filtered)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.bar(range(len(tokens)), scores)\n",
    "    plt.xticks(range(len(tokens)), tokens, rotation=\"vertical\")\n",
    "    plt.title(\"Attention Scores (Layer 0, Head 0)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ========== DataLoader Helper ==========\n",
    "def get_dataloaders(csv_path, label_cols, batch_size=BATCH_SIZE):\n",
    "    df        = pd.read_csv(csv_path)\n",
    "    tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "    tr_df, vl_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "    tr_loader = DataLoader(EmotionDataset(tr_df, tokenizer, label_cols),\n",
    "                           batch_size=batch_size, shuffle=True)\n",
    "    vl_loader = DataLoader(EmotionDataset(vl_df, tokenizer, label_cols),\n",
    "                           batch_size=batch_size)\n",
    "    return tr_loader, vl_loader, tokenizer, tr_df, vl_df\n",
    "\n",
    "# === Part 1: English (ENG) ===\n",
    "english_labels = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\"]\n",
    "train_loader_eng, val_loader_eng, tokenizer, train_df_eng, val_df_eng = \\\n",
    "    get_dataloaders(\"/kaggle/input/languages/eng.csv\", english_labels)\n",
    "\n",
    "train_loader_eng = DataLoader(\n",
    "    EmotionDatasetWithComet(train_df_eng, tokenizer, english_labels, use_comet=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader_eng = DataLoader(\n",
    "    EmotionDatasetWithComet(val_df_eng, tokenizer, english_labels, use_comet=True),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model_eng = XLMREmotionClassifier(num_labels=len(english_labels))\n",
    "metrics_eng = train_model(\n",
    "    model_eng, train_loader_eng, val_loader_eng, english_labels,\n",
    "    loss_type=\"combined\", loss_kwargs={\"alpha\":0.6,\"focal_alpha\":0.25,\"focal_gamma\":2.0},\n",
    "    lr=LR, device=DEVICE\n",
    ")\n",
    "print(\"English validation metrics:\", metrics_eng)\n",
    "\n",
    "sample_eng = val_df_eng.iloc[0][\"text\"]\n",
    "print(\"\\nLIME (ENG):\");    explain_prediction(model_eng, tokenizer, sample_eng, english_labels)\n",
    "print(\"\\nAttention (ENG):\"); plot_attention(model_eng, tokenizer, sample_eng)\n",
    "\n",
    "# === Part 2: Spanish (ESP) ===\n",
    "spanish_labels = [\"anger\",\"fear\",\"joy\",\"sadness\",\"surprise\",\"disgust\"]\n",
    "train_loader_esp, val_loader_esp, tokenizer, train_df_esp, val_df_esp = \\\n",
    "    get_dataloaders(\"/kaggle/input/languages/esp.csv\", spanish_labels)\n",
    "\n",
    "train_loader_esp = DataLoader(\n",
    "    EmotionDatasetWithComet(train_df_esp, tokenizer, spanish_labels, use_comet=True),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader_esp = DataLoader(\n",
    "    EmotionDatasetWithComet(val_df_esp, tokenizer, spanish_labels, use_comet=True),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "model_esp = XLMREmotionClassifier(num_labels=len(spanish_labels))\n",
    "metrics_esp = train_model(\n",
    "    model_esp, train_loader_esp, val_loader_esp, spanish_labels,\n",
    "    loss_type=\"lsbce\", loss_kwargs={\"smoothing\":0.1},\n",
    "    lr=LR, device=DEVICE\n",
    ")\n",
    "print(\"Spanish validation metrics:\", metrics_esp)\n",
    "\n",
    "sample_esp = val_df_esp.iloc[0][\"text\"]\n",
    "print(\"\\nLIME (ESP):\");    explain_prediction(model_esp, tokenizer, sample_esp, spanish_labels)\n",
    "print(\"\\nAttention (ESP):\"); plot_attention(model_esp, tokenizer, sample_esp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7188186,
     "sourceId": 11470230,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
